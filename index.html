<!doctype html>
<html lang="zh-CN">

<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Luo Yu | Senior Researcher</title>
  <link rel="stylesheet" href="styles.css?v=2" />
  <link
    href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;600&family=Lora:ital,wght@0,400;0,600;1,400&display=swap"
    rel="stylesheet">
</head>

<body>
  <div class="page">
    <header class="site-header">
      <div class="site-top">
        <div class="name-block">
          <h1>Luo Yu (ÁΩóÂÆá) <button class="theme-toggle" aria-label="Toggle dark mode"><span
                class="theme-icon theme-icon-light" aria-hidden="true"><svg viewBox="0 0 24 24">
                  <circle cx="12" cy="12" r="5" />
                  <path
                    d="M12 1v2M12 21v2M4.22 4.22l1.42 1.42M18.36 18.36l1.42 1.42M1 12h2M21 12h2M4.22 19.78l1.42-1.42M18.36 5.64l1.42-1.42" />
                </svg></span><span class="theme-icon theme-icon-dark" aria-hidden="true"><svg viewBox="0 0 24 24">
                  <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z" />
                </svg></span></button></h1>
          <p class="subtitle">Senior Researcher @ Huawei Foundation Model Group</p>
        </div>
        <nav class="site-nav">
          <a href="#about">About / Research</a>
          <a href="#experience">Experience</a>
          <a href="#publications">Publications</a>
          <a href="#contact">Contact</a>
        </nav>
      </div>
    </header>

    <div class="hero">
      <div class="hero-portrait">
        <img class="luoyu" src="assets/luoyu.jpg" alt="Luo Yu" />
      </div>
      <div class="hero-text">
        <p class="hero-title">
          Scaling Intelligence through
          <span class="highlight-text">Reinforcement Learning</span>
        </p>
        <p class="hero-desc">
          Bridging the gap between <strong>Reinforcement Learning</strong> and <strong>Foundation Models</strong> to
          unlock complex reasoning and embodied decision-making.
        </p>
        <div class="hero-actions">
          <a class="btn-primary" href="#publications">View Research</a>
          <div class="hero-links">
            <a class="hero-link" href="https://github.com/Roythuly" target="_blank">
              <span class="hero-link-icon" aria-hidden="true">
                <svg viewBox="0 0 16 16" role="presentation">
                  <path
                    d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.08.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.58.82-2.14-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.14 0 3.07-1.87 3.75-3.65 3.95.28.24.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.47.55.38A8.013 8.013 0 0 0 16 8c0-4.42-3.58-8-8-8z">
                  </path>
                </svg>
              </span>
              GitHub
            </a>
            <a class="hero-link" href="https://scholar.google.com/citations?user=KQjoQOMAAAAJ&amp;hl=zh-CN"
              target="_blank">
              <span class="hero-link-icon" aria-hidden="true">
                <svg viewBox="0 0 24 24" role="presentation">
                  <path
                    d="M12 3L2 7l10 4 9-3.64V15l2 1V6L12 0 0 5.5 12 10l8.5-3.45L12 3zm0 9l-9-3 2 4.5V20l7-3 7 3v-6.5L21 9l-9 3z">
                  </path>
                </svg>
              </span>
              Google Scholar
            </a>
          </div>
        </div>
      </div>
    </div>

    <main>
      <section id="about" class="section">
        <div class="section-label">01</div>
        <h2>About Me</h2>
        <div class="about">
          <p>
            I am a <strong>Senior Researcher</strong> at <strong>Huawei Technologies</strong> (Foundation Model Group,
            2012 Labs),
            working on the frontier of <strong>Large Language Model (LLM) Post-training</strong>.
          </p>
          <p>
            I earned my Ph.D. in Department of Computer Science and Technology from <strong>Tsinghua University</strong>
            in 2024, advised by Prof. Fuchun Sun.
          </p>
          <p>
            My research philosophy centers on <strong>Scaling Intelligence</strong>. While deeply rooted in fundamental
            RL algorithms (Online/Offline),
            my current focus is bridging RL with Foundation Models. Specifically, I am dedicated to enhancing the
            <strong>complex reasoning</strong>
            and <strong>agentic decision-making</strong> capabilities of LLMs and Embodied Agents (VLA) via scalable RL.
          </p>
          <p class="about-meta">
            üìç Beijing, China ¬∑ Research Focus: Scalable RL / Agentic RL / Embodied AI
          </p>
          <div class="interest-area">
            <h3>Research Interests</h3>
            <div class="interest-grid">
              <div class="interest-card special-card">
                <div class="recent-title">AI Memory</div>
                <p class="recent-desc">
                  Exploring <strong>model architectures</strong>, <strong>data construction</strong>, and
                  <strong>training algorithms</strong>
                  to equip models with a second core capability beyond reasoning: <strong>memory</strong>.
                </p>
                <div class="tags">
                  <span>Memory Systems</span>
                  <span>Post-training</span>
                  <span>Personalized Agents</span>
                </div>
              </div>
              <div class="interest-card">
                <h4>Scalable RL Algorithms</h4>
                <p>Designing high-efficiency and stable RL algorithms (e.g., OMPO/OBAC/BrHPO) to tackle offline,
                  non-stationary, and hierarchical decision-making challenges.</p>
                <div class="tags">
                  <span>Off-policy RL</span>
                  <span>On-policy RL</span>
                  <span>Hierarchical RL</span>
                </div>
              </div>
              <div class="interest-card">
                <h4>LLM Reasoning & Alignment</h4>
                <p>Unlocking complex reasoning capabilities in Math and Code domains via advanced RLVR and Agentic RL
                  post-training techniques.</p>
                <div class="tags">
                  <span>LLM Post-training</span>
                  <span>RLVR</span>
                  <span>Complex Reasoning</span>
                </div>
              </div>
              <div class="interest-card">
                <h4>Embodied Robots</h4>
                <p>Building Vision-Language-Action (VLA) models capable of robust generalization across diverse physical
                  tasks and seamless sim-to-real transfer.</p>
                <div class="tags">
                  <span>VLA</span>
                  <span>Robotics</span>
                  <span>Sim-2-Real</span>
                </div>
              </div>
            </div>
          </div>
        </div>
      </section>

      <section id="experience" class="section">
        <div class="section-label">02</div>
        <h2>Experience</h2>

        <div class="timeline">
          <div class="item">
            <div class="time">2025.01 - Present</div>
            <div class="detail">
              <div class="role">Senior Researcher ¬∑ Huawei 2012 Labs (Foundation Model Group)</div>
              <div class="desc">
                <strong>Owning RL Post-training</strong> for the <strong>openPangu</strong> model family (1B-718B),
                leading Math and Code reasoning tracks in mainline training.
                <ul style="margin-top:8px; padding-left:16px;">
                  <li>
                    Built and operated the end-to-end RL post-training pipeline, covering infrastructure, training
                    recipes, evaluation, and iterative feedback-driven refinement.
                  </li>
                  <li>
                    Proposed <strong>R¬≤VPO</strong> across Qwen3 and openPangu foundation models.
                  </li>
                </ul>
              </div>

            </div>
          </div>


          <div class="item">
            <div class="time">2019.06 - 2024.12</div>
            <div class="detail">
              <div class="role">Doctoral Researcher ¬∑ Tsinghua University</div>
              <div class="desc">
                Advised by Prof. Fuchun Sun. Focused on <strong>Data-Efficient & Non-stationary RL</strong>.
                <br>
                <ul style="margin-top:4px; padding-left:16px;">
                  <li>Proposed a unified framework for RL under distribution shifts (<strong>ICML 2025 Oral</strong>).
                  </li>
                  <li>Developed a hierarchical RL method for robot exploration.</li>
                </ul>
              </div>
            </div>
          </div>
          <div class="item">
            <div class="time">2022 - 2023</div>
            <div class="detail">
              <div class="role">Research Intern ¬∑ Institute for AI Industry Research (AIR), Tsinghua</div>
              <div class="desc">
                Conducted research on Hierarchical RL for dexterous robotic manipulation.
              </div>
            </div>
          </div>
        </div>
      </section>

      <section id="publications" class="section">
        <div class="section-label">03</div>
        <h2>Selected Publications</h2>
        <div class="pub-list">

          <article class="pub-item">
            <img src="assets/r2vpo_paper.jpg" alt="Arxiv" />
            <div class="pub-meta">
              <div class="pub-title">
                Ratio-Variance Regularized Policy Optimization for Efficient LLM Fine-tuning
              </div>
              <div class="pub-authors"><strong>Yu Luo</strong>, Shuo Han, Yihan Hu, Dong Li, Jianye Hao.</div>
              <div class="pub-venue">
                Under Review / arXiv Preprint</span>, 2026
              </div>
              <a class="pub-link" href="https://arxiv.org/pdf/2601.03320" target="_blank">PDF</a>
            </div>
          </article>

          <article class="pub-item">
            <img src="assets/flowrl_paper.jpg" alt="Arxiv" />
            <div class="pub-meta">
              <div class="pub-title">
                Flow-Based Policy for Online Reinforcement Learning
              </div>
              <div class="pub-authors">Lei Lv, Yunfei Li, <strong>Yu Luo</strong>, Fuchun Sun, Tao Kong, Jiafeng Xu,
                Xiao Ma.</div>
              <div class="pub-venue">
                Advances in Neural Information Processing Systems, 2026
              </div>
              <a class="pub-link" href="https://arxiv.org/pdf/2506.12811" target="_blank">PDF</a>
            </div>
          </article>

          <article class="pub-item">
            <img src="assets/ompo_paper.jpg" alt="ICML 2025" />
            <div class="pub-meta">
              <div class="pub-title">
                OMPO: A Unified Framework for RL under Policy and Dynamics Shifts
              </div>
              <div class="pub-authors"><strong>Yu Luo</strong>, Tianying Ji, Fuchun Sun, Jianwei Zhang, Huazhe Xu,
                Xianyuan Zhan</div>
              <div class="pub-venue">
                International Conference on Machine Learning (ICML), 2025
                <span class="highlight">Oral</span>
              </div>
              <a class="pub-link" href="https://arxiv.org/pdf/2405.19080" target="_blank">PDF</a>
            </div>
          </article>

          <article class="pub-item">
            <img src="assets/obac_paper.jpg" alt="ICML 2025" />
            <div class="pub-meta">
              <div class="pub-title">
                Offline-Boosted Actor-Critic: Adaptively Blending Optimal Historical Behaviors in Deep Off-Policy RL
              </div>
              <div class="pub-authors"><strong>Yu Luo</strong>, Tianying Ji, Fuchun Sun, Jianwei Zhang, Huazhe Xu,
                Xianyuan Zhan</div>
              <div class="pub-venue">
                International Conference on Machine Learning (ICML), 2025
              </div>
              <a class="pub-link" href="https://arxiv.org/pdf/2405.18520" target="_blank">PDF</a>
            </div>
          </article>

          <article class="pub-item">
            <img src="assets/hlmo_paper.jpg" alt="TNNLS" />
            <div class="pub-meta">
              <div class="pub-title">
                Goal-Conditioned Hierarchical Reinforcement Learning with High-Level Model Approximation
              </div>
              <div class="pub-authors"><strong>Yu Luo</strong>, Tianying Ji, Fuchun Sun, Huaping Liu, Jianwei Zhang,
                Mingxuan Jing, Wenbing Huang</div>
              <div class="pub-venue">
                IEEE Transactions on Neural Networks and Learning Systems, 2025
                <span class="highlight">SCI Q1, Top</span>
              </div>
              <a class="pub-link" href="https://ieeexplore.ieee.org/abstract/document/10418512" target="_blank">PDF</a>
            </div>
          </article>

          <article class="pub-item">
            <img src="assets/brhpo_paper.jpg" alt="RLC" />
            <div class="pub-meta">
              <div class="pub-title">
                Bidirectional-Reachable Hierarchical Reinforcement Learning with Mutually Responsive Policies
              </div>
              <div class="pub-authors"><strong>Yu Luo</strong>, Fuchun Sun, Tianying Ji, Xianyuan Zhan</div>
              <div class="pub-venue">
                1st Reinforcement Learning Conference / Reinforcement Learning Journal, 2025
              </div>
              <a class="pub-link" href="https://arxiv.org/pdf/2406.18053" target="_blank">PDF</a>
            </div>
          </article>

          <article class="pub-item">
            <img src="assets/tubempc_paper.jpg" alt="ICRA" />
            <div class="pub-meta">
              <div class="pub-title">
                Smooth Computation without Input Delay: Robust Tube-Based Model Predictive Control for Robot Manipulator
                Planning
              </div>
              <div class="pub-authors"><strong>Yu Luo</strong>, Qie Sima, Tianyin Ji, Fuchun Sun, Huaping Liu, Jianwei
                Zhang</div>
              <div class="pub-venue">
                IEEE International Conference on Robotics and Automation, 2025
              </div>
              <a class="pub-link" href="https://arxiv.org/pdf/2403.01265" target="_blank">PDF</a>
            </div>
          </article>

        </div>
      </section>

      <section id="contact" class="section">
        <div class="section-label">04</div>
        <h2>Get in Touch</h2>
        <p class="contact-intro">
          Interested in collaboration or have questions about my research? I'd love to hear from you.
        </p>
        <div class="contact-cards">
          <a class="contact-card" href="https://www.linkedin.com" target="_blank">LinkedIn</a>
          <a class="contact-card" href="https://scholar.google.com/citations?user=KQjoQOMAAAAJ&hl=zh-CN"
            target="_blank">Google Scholar</a>
          <a class="contact-card" href="https://github.com/Roythuly" target="_blank">GitHub</a>
          <a class="contact-icon" href="mailto:roythu95@gmail.com" aria-label="Send email to Luo Yu">‚úâ</a>
        </div>
      </section>
    </main>

    <footer class="site-footer">
      <div class="footer-brand">LY</div>
      <div class="footer-links">
        <a href="#experience">Experience</a>
        <a href="#publications">Publications</a>
        <a href="#contact">Contact</a>
      </div>
      <span>¬© 2026 Luo Yu. All rights reserved.</span>
    </footer>
    <button class="scroll-top" aria-label="Return to top">
      <span class="scroll-top-icon" aria-hidden="true">
        <svg viewBox="0 0 24 24">
          <path d="M12 5l-7 12h14z" />
        </svg>
      </span>
    </button>
  </div>
  <script>
    const scrollBtn = document.querySelector('.scroll-top');
    scrollBtn.addEventListener('click', () => window.scrollTo({ top: 0, behavior: 'smooth' }));

    // Theme toggle
    const themeToggle = document.querySelector('.theme-toggle');
    const savedTheme = localStorage.getItem('theme') || 'light';
    if (savedTheme === 'dark') {
      document.documentElement.setAttribute('data-theme', 'dark');
    }
    themeToggle.addEventListener('click', () => {
      const current = document.documentElement.getAttribute('data-theme');
      const next = current === 'dark' ? 'light' : 'dark';
      document.documentElement.setAttribute('data-theme', next === 'dark' ? 'dark' : '');
      localStorage.setItem('theme', next);
    });
  </script>
</body>

</html>
